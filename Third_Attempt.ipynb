{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Create the folder\n",
        "# !mkdir -p \"/content/drive/MyDrive/kaggle_heart_disease\""
      ],
      "metadata": {
        "id": "4BzbRTeffvRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d20f77-f36b-4ff5-89dc-3ee83fcfb986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/kaggle_heart_disease/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/kaggle_heart_disease/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/kaggle_heart_disease/sample_submission.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Sample submission shape: {sample_submission.shape}\")"
      ],
      "metadata": {
        "id": "1hBi5HfafvJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dd8ae0-8047-47bc-f9a7-89b6da0d4d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (630000, 15)\n",
            "Test shape: (270000, 14)\n",
            "Sample submission shape: (270000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "train['Heart Disease'] = train['Heart Disease'].map({'Absence': 0, 'Presence': 1})\n",
        "X = train.drop(['id', 'Heart Disease'], axis=1)\n",
        "y = train['Heart Disease']\n",
        "\n",
        "# Keep test IDs for submission\n",
        "test_ids = test['id']\n",
        "X_test = test.drop(['id'], axis=1)"
      ],
      "metadata": {
        "id": "V7jZqcXUfvA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = [\n",
        "    'Sex',                # binary\n",
        "    'Chest pain type',    # ordinal (1-4)\n",
        "    'FBS over 120',       # binary\n",
        "    'EKG results',        # ordinal (0-2)\n",
        "    'Exercise angina',    # binary\n",
        "    'Slope of ST',        # ordinal (1-3)\n",
        "    'Number of vessels fluro',  # ordinal (0-3)\n",
        "    'Thallium'            # ordinal (3,6,7)\n",
        "]\n",
        "\n",
        "numerical_features = [\n",
        "    'Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression'\n",
        "]"
      ],
      "metadata": {
        "id": "56J209cvfu4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "lgb_cv_scores = []\n",
        "\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\nüî• LightGBM Fold {fold+1}\")\n",
        "\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    model = lgb.train(\n",
        "        lgb_params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[val_data],\n",
        "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    auc = roc_auc_score(y_val, y_pred)\n",
        "    lgb_cv_scores.append(auc)\n",
        "    print(f\" Fold {fold+1} ROC AUC: {auc:.5f}\")\n",
        "\n",
        "print(f\"\\n LightGBM CV: {np.mean(lgb_cv_scores):.5f} ¬± {np.std(lgb_cv_scores):.5f}\")"
      ],
      "metadata": {
        "id": "ZpIoY1P-futo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996cc187-976a-4bfd-c637-cebd050d64ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî• LightGBM Fold 1\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[638]\tvalid_0's auc: 0.95562\n",
            " Fold 1 ROC AUC: 0.95562\n",
            "\n",
            "üî• LightGBM Fold 2\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[430]\tvalid_0's auc: 0.95462\n",
            " Fold 2 ROC AUC: 0.95462\n",
            "\n",
            "üî• LightGBM Fold 3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[404]\tvalid_0's auc: 0.955391\n",
            " Fold 3 ROC AUC: 0.95539\n",
            "\n",
            "üî• LightGBM Fold 4\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[452]\tvalid_0's auc: 0.954927\n",
            " Fold 4 ROC AUC: 0.95493\n",
            "\n",
            "üî• LightGBM Fold 5\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[510]\tvalid_0's auc: 0.955754\n",
            " Fold 5 ROC AUC: 0.95575\n",
            "\n",
            " LightGBM CV: 0.95526 ¬± 0.00043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --q\n",
        "\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Make sure X and y are loaded from previous cells\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 3)\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        **params,\n",
        "        n_estimators=500,                # fixed number (no early stopping needed)\n",
        "        eval_metric='auc',\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    score = np.mean(cross_val_score(model, X, y, cv=3, scoring='roc_auc'))\n",
        "    return score\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)   # 50 trials ~ 30-60 min depending on data size\n",
        "\n",
        "print(\"Best params:\", study.best_params)\n",
        "print(f\"Best CV: {study.best_value:.5f}\")"
      ],
      "metadata": {
        "id": "yomVcbrGfuhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e8f10d-ebcb-4bfc-d651-959bbb1415eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/413.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-02-21 11:25:14,769] A new study created in memory with name: no-name-ece50519-1ffd-463e-821b-1e0f0fb7dc54\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:25:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:25:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:26:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:26:44,227] Trial 0 finished with value: 0.9499187382196211 and parameters: {'learning_rate': 0.17414750101433732, 'max_depth': 11, 'subsample': 0.7723951531704004, 'colsample_bytree': 0.5406493114387956, 'min_child_weight': 6, 'gamma': 0.44254787106921967, 'reg_alpha': 0.6398258358162154, 'reg_lambda': 1.7444558218716912}. Best is trial 0 with value: 0.9499187382196211.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:26:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:27:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:27:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:27:55,856] Trial 1 finished with value: 0.9529433536408578 and parameters: {'learning_rate': 0.19953128728203853, 'max_depth': 7, 'subsample': 0.5556046178930298, 'colsample_bytree': 0.6563950346610015, 'min_child_weight': 10, 'gamma': 0.07379630125297088, 'reg_alpha': 0.5529119056193441, 'reg_lambda': 1.2672396360744123}. Best is trial 1 with value: 0.9529433536408578.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:27:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:28:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:28:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:29:01,576] Trial 2 finished with value: 0.9544458516569431 and parameters: {'learning_rate': 0.09056814259950154, 'max_depth': 7, 'subsample': 0.7730044434176226, 'colsample_bytree': 0.7798179029427144, 'min_child_weight': 1, 'gamma': 0.06341154963651191, 'reg_alpha': 0.13129662706401868, 'reg_lambda': 1.6315016485371128}. Best is trial 2 with value: 0.9544458516569431.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:29:02] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:29:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:29:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:30:28,641] Trial 3 finished with value: 0.9494736334146241 and parameters: {'learning_rate': 0.1814681295728542, 'max_depth': 10, 'subsample': 0.8207725174024096, 'colsample_bytree': 0.8536468316837158, 'min_child_weight': 8, 'gamma': 0.010831325348206189, 'reg_alpha': 0.18230200139414543, 'reg_lambda': 2.9258130247867715}. Best is trial 2 with value: 0.9544458516569431.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:30:29] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:30:44] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:30:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:31:14,498] Trial 4 finished with value: 0.9551299470274502 and parameters: {'learning_rate': 0.1753448979312988, 'max_depth': 4, 'subsample': 0.8456145597714664, 'colsample_bytree': 0.8888957237794599, 'min_child_weight': 10, 'gamma': 0.42030850912251094, 'reg_alpha': 0.3313322635717516, 'reg_lambda': 1.5310807012816547}. Best is trial 4 with value: 0.9551299470274502.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:31:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:31:26] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:31:39] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:31:52,322] Trial 5 finished with value: 0.9554186393015173 and parameters: {'learning_rate': 0.14141741453746878, 'max_depth': 3, 'subsample': 0.9179359652893173, 'colsample_bytree': 0.6901165881267535, 'min_child_weight': 9, 'gamma': 0.05213180372025816, 'reg_alpha': 0.6114318353368028, 'reg_lambda': 2.0173881469385564}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:31:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:32:19] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:32:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:33:13,102] Trial 6 finished with value: 0.9539787272557083 and parameters: {'learning_rate': 0.07572168200143813, 'max_depth': 8, 'subsample': 0.5586329353256294, 'colsample_bytree': 0.8681598590090912, 'min_child_weight': 1, 'gamma': 0.3304487051538597, 'reg_alpha': 0.3236385667714007, 'reg_lambda': 1.7083325836721808}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:33:13] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:33:35] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:33:55] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:34:16,807] Trial 7 finished with value: 0.9552296154044919 and parameters: {'learning_rate': 0.03331649899980267, 'max_depth': 6, 'subsample': 0.8558287232023044, 'colsample_bytree': 0.6687284668625663, 'min_child_weight': 9, 'gamma': 0.150985048830778, 'reg_alpha': 0.6781164083477734, 'reg_lambda': 1.4105716388554517}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:34:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:34:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:34:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:35:05,867] Trial 8 finished with value: 0.9550970696058281 and parameters: {'learning_rate': 0.16378585842613164, 'max_depth': 4, 'subsample': 0.7195548911425113, 'colsample_bytree': 0.9292241578821641, 'min_child_weight': 1, 'gamma': 0.1530011826369812, 'reg_alpha': 0.012623250588451596, 'reg_lambda': 1.575875005164437}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:35:06] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:35:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:35:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:35:48,603] Trial 9 finished with value: 0.9553829672361269 and parameters: {'learning_rate': 0.11148286028898242, 'max_depth': 3, 'subsample': 0.7878268353625695, 'colsample_bytree': 0.9163021158957131, 'min_child_weight': 9, 'gamma': 0.3052733976415532, 'reg_alpha': 0.5837578254431436, 'reg_lambda': 1.7008685415164304}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:35:49] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:36:03] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:36:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:36:30,694] Trial 10 finished with value: 0.9553856574296442 and parameters: {'learning_rate': 0.13401411334931043, 'max_depth': 5, 'subsample': 0.9979134360262585, 'colsample_bytree': 0.5338964093372828, 'min_child_weight': 5, 'gamma': 0.17004320424556912, 'reg_alpha': 0.9574449506956597, 'reg_lambda': 2.4687854125365387}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:36:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:36:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:37:00] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:37:15,375] Trial 11 finished with value: 0.9553520662971863 and parameters: {'learning_rate': 0.12792149876051787, 'max_depth': 5, 'subsample': 0.9908202643916236, 'colsample_bytree': 0.5120050432034897, 'min_child_weight': 4, 'gamma': 0.1759887289525287, 'reg_alpha': 0.9907444709669487, 'reg_lambda': 2.3961898335865306}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:37:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:37:29] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:37:42] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:37:54,797] Trial 12 finished with value: 0.9554163575687814 and parameters: {'learning_rate': 0.13544074314920573, 'max_depth': 3, 'subsample': 0.995821235894927, 'colsample_bytree': 0.6218258242187894, 'min_child_weight': 5, 'gamma': 0.22963476573968533, 'reg_alpha': 0.9226220106798949, 'reg_lambda': 2.276795556010137}. Best is trial 5 with value: 0.9554186393015173.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:37:55] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:38:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:38:22] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:38:35,319] Trial 13 finished with value: 0.95542504635716 and parameters: {'learning_rate': 0.13801915871442338, 'max_depth': 3, 'subsample': 0.9132589325186952, 'colsample_bytree': 0.6474663795610507, 'min_child_weight': 7, 'gamma': 0.25055109010939214, 'reg_alpha': 0.804602287250757, 'reg_lambda': 2.1605446126948986}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:38:35] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:39:02] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:39:27] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:39:51,659] Trial 14 finished with value: 0.9545971718641292 and parameters: {'learning_rate': 0.055007031308847605, 'max_depth': 9, 'subsample': 0.9100838147788083, 'colsample_bytree': 0.7455177118594636, 'min_child_weight': 7, 'gamma': 0.33618532550777575, 'reg_alpha': 0.7931945234141653, 'reg_lambda': 2.0673401328428667}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:39:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:40:28] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:41:06] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:41:42,310] Trial 15 finished with value: 0.9485214486080348 and parameters: {'learning_rate': 0.14712641373670987, 'max_depth': 12, 'subsample': 0.6806058294193233, 'colsample_bytree': 0.7343099281521416, 'min_child_weight': 7, 'gamma': 0.2405983428757651, 'reg_alpha': 0.7745559792032695, 'reg_lambda': 2.7366668011683544}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:41:42] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:41:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:42:10] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:42:23,846] Trial 16 finished with value: 0.955414966074399 and parameters: {'learning_rate': 0.10803280718632507, 'max_depth': 3, 'subsample': 0.9247380012709017, 'colsample_bytree': 0.5941267377590949, 'min_child_weight': 8, 'gamma': 0.3888297492798285, 'reg_alpha': 0.4417332392470406, 'reg_lambda': 2.0693161141143754}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:42:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:42:42] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:43:00] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:43:17,306] Trial 17 finished with value: 0.9549916110263172 and parameters: {'learning_rate': 0.14638372787556825, 'max_depth': 5, 'subsample': 0.918133540504556, 'colsample_bytree': 0.6979663580666872, 'min_child_weight': 3, 'gamma': 0.09048250908952205, 'reg_alpha': 0.8170325212128055, 'reg_lambda': 1.0530601434660416}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:43:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:43:35] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:43:53] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:44:10,703] Trial 18 finished with value: 0.9553235896252978 and parameters: {'learning_rate': 0.082737946081721, 'max_depth': 4, 'subsample': 0.6370646762595902, 'colsample_bytree': 0.8071462909886504, 'min_child_weight': 7, 'gamma': 0.29601307947479055, 'reg_alpha': 0.44534849549720534, 'reg_lambda': 1.9430035325194404}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:44:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:44:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:44:49] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:45:09,182] Trial 19 finished with value: 0.9547056687302012 and parameters: {'learning_rate': 0.1154914504488854, 'max_depth': 6, 'subsample': 0.87779660359786, 'colsample_bytree': 0.994748879888099, 'min_child_weight': 9, 'gamma': 0.010034925965990607, 'reg_alpha': 0.7167918404401423, 'reg_lambda': 2.59055372499014}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:45:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:45:27] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:45:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:46:05,135] Trial 20 finished with value: 0.9544791225973953 and parameters: {'learning_rate': 0.19859829649070862, 'max_depth': 6, 'subsample': 0.9421635401568998, 'colsample_bytree': 0.5703645734071644, 'min_child_weight': 6, 'gamma': 0.2241805732305811, 'reg_alpha': 0.8446334871434792, 'reg_lambda': 2.2596097199254244}. Best is trial 13 with value: 0.95542504635716.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:46:05] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:46:19] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:46:32] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:46:44,782] Trial 21 finished with value: 0.9554539185762435 and parameters: {'learning_rate': 0.15145680976999654, 'max_depth': 3, 'subsample': 0.978942255154114, 'colsample_bytree': 0.6214676665694582, 'min_child_weight': 4, 'gamma': 0.22355976956356083, 'reg_alpha': 0.8760207805750355, 'reg_lambda': 2.247176656391137}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:46:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:46:57] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:47:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:47:22,322] Trial 22 finished with value: 0.9554232552629284 and parameters: {'learning_rate': 0.15347259833001017, 'max_depth': 3, 'subsample': 0.9513793710233741, 'colsample_bytree': 0.6474932375057478, 'min_child_weight': 3, 'gamma': 0.27259917846789206, 'reg_alpha': 0.891375194149975, 'reg_lambda': 1.9139471810960513}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:47:22] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:47:37] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:47:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:48:06,307] Trial 23 finished with value: 0.9553638010329619 and parameters: {'learning_rate': 0.16017418261000374, 'max_depth': 4, 'subsample': 0.952875013870347, 'colsample_bytree': 0.6057824704362008, 'min_child_weight': 3, 'gamma': 0.2770963983318746, 'reg_alpha': 0.8913418640976404, 'reg_lambda': 2.202933004123736}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:48:07] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:48:19] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:48:32] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:48:45,483] Trial 24 finished with value: 0.9554283070893969 and parameters: {'learning_rate': 0.1553256976001421, 'max_depth': 3, 'subsample': 0.96154616771542, 'colsample_bytree': 0.6279956272865682, 'min_child_weight': 3, 'gamma': 0.36630531334499483, 'reg_alpha': 0.7348994074083064, 'reg_lambda': 1.8692298805619227}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:48:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:49:03] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:49:21] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:49:37,825] Trial 25 finished with value: 0.9552494619081454 and parameters: {'learning_rate': 0.12397463959015681, 'max_depth': 5, 'subsample': 0.8782348389569824, 'colsample_bytree': 0.5660267841819336, 'min_child_weight': 4, 'gamma': 0.4989341103842133, 'reg_alpha': 0.7272894022539864, 'reg_lambda': 2.603930699108473}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:49:38] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:49:54] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:50:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:50:24,931] Trial 26 finished with value: 0.9553653735093887 and parameters: {'learning_rate': 0.09521317233403566, 'max_depth': 4, 'subsample': 0.8253910048252528, 'colsample_bytree': 0.716364385785285, 'min_child_weight': 2, 'gamma': 0.3707584552324368, 'reg_alpha': 0.7207331432819594, 'reg_lambda': 1.8455873568231673}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:50:25] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:50:40] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:50:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:51:11,543] Trial 27 finished with value: 0.9552836556067015 and parameters: {'learning_rate': 0.18328866908041608, 'max_depth': 3, 'subsample': 0.5010304259127641, 'colsample_bytree': 0.6335564165574196, 'min_child_weight': 4, 'gamma': 0.19766140800242082, 'reg_alpha': 0.8479352576465179, 'reg_lambda': 2.163967767000326}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:51:12] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:51:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:51:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:52:17,280] Trial 28 finished with value: 0.9528737713695538 and parameters: {'learning_rate': 0.15853180934340325, 'max_depth': 8, 'subsample': 0.9693908584727429, 'colsample_bytree': 0.7808721463681546, 'min_child_weight': 2, 'gamma': 0.11200803537003456, 'reg_alpha': 0.5029851953610752, 'reg_lambda': 2.3773306814424453}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:52:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:52:40] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:53:01] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:53:21,623] Trial 29 finished with value: 0.952361999768184 and parameters: {'learning_rate': 0.16586835532975092, 'max_depth': 11, 'subsample': 0.8923233011810198, 'colsample_bytree': 0.5862873937816399, 'min_child_weight': 6, 'gamma': 0.4622401943220774, 'reg_alpha': 0.6483257928402464, 'reg_lambda': 1.8252620529609684}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:53:22] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:53:38] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:53:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:54:12,991] Trial 30 finished with value: 0.9554064838002372 and parameters: {'learning_rate': 0.12203334398023062, 'max_depth': 4, 'subsample': 0.7320720016091598, 'colsample_bytree': 0.547302766695888, 'min_child_weight': 5, 'gamma': 0.4188187640900628, 'reg_alpha': 0.7764325435437984, 'reg_lambda': 2.8319612681999815}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:54:13] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:54:26] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:54:38] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:54:49,992] Trial 31 finished with value: 0.9554400956738259 and parameters: {'learning_rate': 0.1511823602047657, 'max_depth': 3, 'subsample': 0.9600840919699026, 'colsample_bytree': 0.6447541138757431, 'min_child_weight': 3, 'gamma': 0.26203393051677415, 'reg_alpha': 0.8990658128640222, 'reg_lambda': 1.88557251523007}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:54:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:55:03] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:55:16] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:55:29,291] Trial 32 finished with value: 0.9554117260088323 and parameters: {'learning_rate': 0.16968453320632704, 'max_depth': 3, 'subsample': 0.9749122632197452, 'colsample_bytree': 0.67512861425663, 'min_child_weight': 2, 'gamma': 0.3448569365274128, 'reg_alpha': 0.9235562498788882, 'reg_lambda': 2.12494509581769}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:55:29] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:55:44] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:55:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:56:13,970] Trial 33 finished with value: 0.9552542218908574 and parameters: {'learning_rate': 0.18914003407707977, 'max_depth': 4, 'subsample': 0.9529050333529424, 'colsample_bytree': 0.6401809403713373, 'min_child_weight': 4, 'gamma': 0.26748018348187536, 'reg_alpha': 0.977780984890871, 'reg_lambda': 1.8517968032621486}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:56:14] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:56:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:56:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:57:06,353] Trial 34 finished with value: 0.955030375887036 and parameters: {'learning_rate': 0.15095147855338903, 'max_depth': 5, 'subsample': 0.8963143897265408, 'colsample_bytree': 0.6190813154766047, 'min_child_weight': 3, 'gamma': 0.20517715893968866, 'reg_alpha': 0.874253818373525, 'reg_lambda': 1.9713178083243015}. Best is trial 21 with value: 0.9554539185762435.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:57:06] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:57:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:57:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:57:48,094] Trial 35 finished with value: 0.9554567691185549 and parameters: {'learning_rate': 0.17489310273213515, 'max_depth': 3, 'subsample': 0.8061033381348058, 'colsample_bytree': 0.5005378249476387, 'min_child_weight': 6, 'gamma': 0.31265590244973773, 'reg_alpha': 0.774276481468894, 'reg_lambda': 2.5149064109050525}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:57:48] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:58:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:58:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:58:53,642] Trial 36 finished with value: 0.9539824290722175 and parameters: {'learning_rate': 0.17620205227067895, 'max_depth': 7, 'subsample': 0.80324130729577, 'colsample_bytree': 0.5068387153741293, 'min_child_weight': 6, 'gamma': 0.31329770322681444, 'reg_alpha': 0.751381226807438, 'reg_lambda': 2.5122004497372634}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:58:54] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:59:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:59:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 11:59:38,828] Trial 37 finished with value: 0.9552528093991185 and parameters: {'learning_rate': 0.1922100220112633, 'max_depth': 4, 'subsample': 0.861389763003707, 'colsample_bytree': 0.5457627696753639, 'min_child_weight': 4, 'gamma': 0.3496322801870019, 'reg_alpha': 0.6709548803853517, 'reg_lambda': 1.4773376617768887}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:59:39] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [11:59:54] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:00:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:00:22,110] Trial 38 finished with value: 0.9553354153497923 and parameters: {'learning_rate': 0.1769038531883385, 'max_depth': 3, 'subsample': 0.6522409726869669, 'colsample_bytree': 0.7843078326382592, 'min_child_weight': 5, 'gamma': 0.39905742747096873, 'reg_alpha': 0.5319961504530223, 'reg_lambda': 1.360701988215194}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:00:22] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:00:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:01:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:01:30,990] Trial 39 finished with value: 0.9547086806472599 and parameters: {'learning_rate': 0.018522049516107225, 'max_depth': 6, 'subsample': 0.8382978841727888, 'colsample_bytree': 0.7031632016099463, 'min_child_weight': 2, 'gamma': 0.4434048274393688, 'reg_alpha': 0.9275779315947779, 'reg_lambda': 2.953037624934063}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:01:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:01:48] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:02:04] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:02:19,822] Trial 40 finished with value: 0.9553255764190861 and parameters: {'learning_rate': 0.10110897840156567, 'max_depth': 4, 'subsample': 0.7577297743718264, 'colsample_bytree': 0.8195575861709864, 'min_child_weight': 3, 'gamma': 0.37346246228744756, 'reg_alpha': 0.5825469841612597, 'reg_lambda': 1.6474681129588786}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:02:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:02:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:02:47] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:03:01,038] Trial 41 finished with value: 0.9554262265149012 and parameters: {'learning_rate': 0.14019890333150564, 'max_depth': 3, 'subsample': 0.9307820054007667, 'colsample_bytree': 0.6668735894516088, 'min_child_weight': 8, 'gamma': 0.244824697872067, 'reg_alpha': 0.8304835046942922, 'reg_lambda': 2.3235578096830394}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:03:01] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:03:14] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:03:27] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:03:40,715] Trial 42 finished with value: 0.9554347205856105 and parameters: {'learning_rate': 0.15701343873567333, 'max_depth': 3, 'subsample': 0.970306575326008, 'colsample_bytree': 0.6793463195520114, 'min_child_weight': 10, 'gamma': 0.2956839409790381, 'reg_alpha': 0.8442624148813938, 'reg_lambda': 2.3124354124284943}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:03:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:03:54] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:04:07] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:04:20,163] Trial 43 finished with value: 0.9554062045624162 and parameters: {'learning_rate': 0.16976658484397428, 'max_depth': 3, 'subsample': 0.9660968747860643, 'colsample_bytree': 0.7247662400728376, 'min_child_weight': 10, 'gamma': 0.30276386097118574, 'reg_alpha': 0.7000909479618206, 'reg_lambda': 2.673263797688948}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:04:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:04:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:04:48] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:05:01,818] Trial 44 finished with value: 0.9553014351180426 and parameters: {'learning_rate': 0.1583308665306401, 'max_depth': 4, 'subsample': 0.9750232272075711, 'colsample_bytree': 0.682481392082968, 'min_child_weight': 1, 'gamma': 0.3207513362295965, 'reg_alpha': 0.8517595432873593, 'reg_lambda': 2.455973795863871}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:05:02] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:05:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:05:28] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:05:40,868] Trial 45 finished with value: 0.9554469835787288 and parameters: {'learning_rate': 0.13089343675450188, 'max_depth': 3, 'subsample': 0.9940714951475913, 'colsample_bytree': 0.5799614751988713, 'min_child_weight': 4, 'gamma': 0.2874291554730992, 'reg_alpha': 0.6214094338208582, 'reg_lambda': 1.7711390794817734}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:05:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:05:53] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:06:05] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:06:17,915] Trial 46 finished with value: 0.9553877560400602 and parameters: {'learning_rate': 0.1304718366968797, 'max_depth': 5, 'subsample': 0.9994264712600321, 'colsample_bytree': 0.5208695741313671, 'min_child_weight': 4, 'gamma': 0.28750806553134345, 'reg_alpha': 0.9539284174020652, 'reg_lambda': 1.770422462121589}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:06:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:06:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:07:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:07:51,069] Trial 47 finished with value: 0.9517121423401228 and parameters: {'learning_rate': 0.14359183135752168, 'max_depth': 10, 'subsample': 0.6007777478662757, 'colsample_bytree': 0.5783810680028587, 'min_child_weight': 10, 'gamma': 0.21103280848658929, 'reg_alpha': 0.6102111963946353, 'reg_lambda': 2.0385772111383216}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:07:51] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:08:04] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:08:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:08:31,202] Trial 48 finished with value: 0.955409498339861 and parameters: {'learning_rate': 0.1821067558665062, 'max_depth': 3, 'subsample': 0.9350587136178881, 'colsample_bytree': 0.6055868329499245, 'min_child_weight': 5, 'gamma': 0.2649915066060088, 'reg_alpha': 0.27972770850433204, 'reg_lambda': 2.3166111867203294}. Best is trial 35 with value: 0.9554567691185549.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:08:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:08:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [12:09:01] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-21 12:09:14,739] Trial 49 finished with value: 0.9553941900882174 and parameters: {'learning_rate': 0.16763346157090694, 'max_depth': 4, 'subsample': 0.9900104406641064, 'colsample_bytree': 0.53157258026775, 'min_child_weight': 5, 'gamma': 0.1879797796622799, 'reg_alpha': 0.6417896498387113, 'reg_lambda': 1.6058378361848686}. Best is trial 35 with value: 0.9554567691185549.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'learning_rate': 0.17489310273213515, 'max_depth': 3, 'subsample': 0.8061033381348058, 'colsample_bytree': 0.5005378249476387, 'min_child_weight': 6, 'gamma': 0.31265590244973773, 'reg_alpha': 0.774276481468894, 'reg_lambda': 2.5149064109050525}\n",
            "Best CV: 0.95546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study.best_params.copy()\n",
        "best_params.pop('n_estimators', None)\n",
        "best_params['n_estimators'] = 1000  # we'll use early stopping to find optimal number\n",
        "\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    early_stopping_rounds=50,\n",
        "    eval_metric='auc',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# We need an eval_set for early stopping ‚Äî split a small validation set from training\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "final_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"Best iteration: {final_model.best_iteration}\")"
      ],
      "metadata": {
        "id": "0G44Qrcxftaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749aa863-a7fb-45ad-83fb-04c485d466db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best iteration: 407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From your correlation list:\n",
        "top_features = ['Thallium', 'Chest pain type', 'Exercise angina',\n",
        "                'Number of vessels fluro', 'ST depression', 'Slope of ST', 'Max HR']\n",
        "\n",
        "# Interactions (multiplicative)\n",
        "X['thal_x_chest'] = X['Thallium'] * X['Chest pain type']\n",
        "X['exang_x_vessels'] = X['Exercise angina'] * X['Number of vessels fluro']\n",
        "X['stdep_x_slope'] = X['ST depression'] * X['Slope of ST']\n",
        "X['age_x_maxhr'] = X['Age'] * X['Max HR']\n",
        "\n",
        "# Ratios (if denominator > 0)\n",
        "X['bp_per_age'] = X['BP'] / (X['Age'] + 1)\n",
        "X['chol_per_age'] = X['Cholesterol'] / (X['Age'] + 1)\n",
        "\n",
        "# Polynomials (degree=2) for top 3\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
        "top3 = X[['Thallium', 'Chest pain type', 'Exercise angina']]\n",
        "poly_features = poly.fit_transform(top3)\n",
        "poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(top3.columns))\n",
        "X = pd.concat([X, poly_df], axis=1)"
      ],
      "metadata": {
        "id": "qxbvTzpKfhAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "\n",
        "# --- XGBoost best params ---\n",
        "best_xgb_params = study.best_params.copy()\n",
        "best_xgb_params.pop('n_estimators', None)   # ensure no conflict\n",
        "\n",
        "# --- LightGBM params (defaults for now) ---\n",
        "best_lgb_params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_child_samples': 20,\n",
        "    'reg_alpha': 0.0,\n",
        "    'reg_lambda': 0.0,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "# Split data for early stopping\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "# Train XGBoost with early stopping\n",
        "xgb_final = xgb.XGBClassifier(**best_xgb_params, n_estimators=1000,\n",
        "                              early_stopping_rounds=50, random_state=42, n_jobs=-1)\n",
        "xgb_final.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "# Train LightGBM with early stopping\n",
        "lgb_final = lgb.LGBMClassifier(**best_lgb_params, n_estimators=1000,\n",
        "                               early_stopping_rounds=50, random_state=42, n_jobs=-1)\n",
        "lgb_final.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "# Get optimal number of trees\n",
        "xgb_best_iter = xgb_final.best_iteration\n",
        "lgb_best_iter = lgb_final.best_iteration_\n",
        "\n",
        "print(f\"XGBoost optimal trees: {xgb_best_iter}\")\n",
        "print(f\"LightGBM optimal trees: {lgb_best_iter}\")\n",
        "\n",
        "# Retrain on full data with optimal trees\n",
        "xgb_full = xgb.XGBClassifier(**best_xgb_params, n_estimators=xgb_best_iter, random_state=42, n_jobs=-1)\n",
        "xgb_full.fit(X, y)\n",
        "\n",
        "lgb_full = lgb.LGBMClassifier(**best_lgb_params, n_estimators=lgb_best_iter, random_state=42, n_jobs=-1)\n",
        "lgb_full.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "xgb_preds = xgb_full.predict_proba(X_test)[:, 1]\n",
        "lgb_preds = lgb_full.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Ensemble\n",
        "ensemble_preds = (xgb_preds + lgb_preds) / 2\n",
        "\n",
        "# Submission\n",
        "submission = pd.DataFrame({'id': test_ids, 'Heart Disease': ensemble_preds})\n",
        "submission.to_csv('ensemble_xgb_lgb.csv', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('ensemble_xgb_lgb.csv')"
      ],
      "metadata": {
        "id": "d-Lmxoqufdbi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f44d224a-1628-486b-e5a4-18ccad955996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'dtype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1634039548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m xgb_final = xgb.XGBClassifier(**best_xgb_params, n_estimators=1000,\n\u001b[1;32m     28\u001b[0m                               early_stopping_rounds=50, random_state=42, n_jobs=-1)\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mxgb_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train LightGBM with early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEvalsLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1788\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \"\"\"\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Feature_types contains the optional reference categories from the booster object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gblinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                 return QuantileDMatrix(\n\u001b[0m\u001b[1;32m   1258\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 )\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m         self._init(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[1;32m   1781\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         )\n\u001b[0;32m-> 1783\u001b[0;31m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m         \u001b[0;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m  \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;31m# Defer the exception in order to return 0 and stop the iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0minput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minput_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                 new, feature_names, feature_types = _proxy_transform(\n\u001b[0m\u001b[1;32m    643\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_proxy_transform\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf_pa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         df, feature_names, feature_types = _transform_pandas_df(\n\u001b[0m\u001b[1;32m   1696\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[0m\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_transform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m     return (\n\u001b[1;32m    674\u001b[0m         \u001b[0mPandasTransformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_categories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mpandas_transform_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moth_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# FIXME(jiamingy): Investigate the possibility of using dataframe protocol or arrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36moth_type\u001b[0;34m(ser)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# The dtypes module is added in 1.25.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         npdtypes = np_dtypes and isinstance(\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             (\n\u001b[1;32m    569\u001b[0m                 \u001b[0;31m# pylint: disable=no-member\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dtype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvpavOJXfJA4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import catboost as cb\n",
        "\n",
        "base_models = [\n",
        "    ('xgb', xgb.XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)),\n",
        "    ('lgb', lgb.LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=31, random_state=42)),\n",
        "    ('cb', cb.CatBoostClassifier(iterations=500, learning_rate=0.05, verbose=0, random_state=42))\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=5)\n",
        "stack.fit(X, y)\n",
        "stack_preds = stack.predict_proba(X_test)[:, 1]\n",
        "\n",
        "submission['Heart Disease'] = stack_preds\n",
        "submission.to_csv('stacking_submission.csv', index=False)"
      ]
    }
  ]
}