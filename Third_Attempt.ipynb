{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Create the folder\n",
        "# !mkdir -p \"/content/drive/MyDrive/kaggle_heart_disease\""
      ],
      "metadata": {
        "id": "4BzbRTeffvRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec4f610-df78-44d8-faf7-e5ea144cbd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/kaggle_heart_disease/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/kaggle_heart_disease/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/kaggle_heart_disease/sample_submission.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Sample submission shape: {sample_submission.shape}\")"
      ],
      "metadata": {
        "id": "1hBi5HfafvJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25153f3-9de1-481b-bb51-b439ad88a542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (630000, 15)\n",
            "Test shape: (270000, 14)\n",
            "Sample submission shape: (270000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "train['Heart Disease'] = train['Heart Disease'].map({'Absence': 0, 'Presence': 1})\n",
        "X = train.drop(['id', 'Heart Disease'], axis=1)\n",
        "y = train['Heart Disease']\n",
        "\n",
        "# Keep test IDs for submission\n",
        "test_ids = test['id']\n",
        "X_test = test.drop(['id'], axis=1)"
      ],
      "metadata": {
        "id": "V7jZqcXUfvA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hau2GG0bv0BS",
        "outputId": "de763962-3f1e-40b3-f351-15b5b58d9450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Age', 'Sex', 'Chest pain type', 'BP', 'Cholesterol', 'FBS over 120', 'EKG results', 'Max HR', 'Exercise angina', 'ST depression', 'Slope of ST', 'Number of vessels fluro', 'Thallium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = [\n",
        "    'Sex',                # binary\n",
        "    'Chest pain type',    # ordinal (1-4)\n",
        "    'FBS over 120',       # binary\n",
        "    'EKG results',        # ordinal (0-2)\n",
        "    'Exercise angina',    # binary\n",
        "    'Slope of ST',        # ordinal (1-3)\n",
        "    'Number of vessels fluro',  # ordinal (0-3)\n",
        "    'Thallium'            # ordinal (3,6,7)\n",
        "]\n",
        "\n",
        "numerical_features = [\n",
        "    'Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression'\n",
        "]"
      ],
      "metadata": {
        "id": "56J209cvfu4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = X.columns[X.columns.duplicated()].tolist()\n",
        "print(f\"Duplicate columns: {duplicates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaUI0S_JDmp6",
        "outputId": "8a4dc50c-793c-4080-fc93-1450d4616250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate columns: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.dtypes.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0xa5bYkFCIS",
        "outputId": "94c502f6-5305-44ed-8c71-d3e50c2cc0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int64      12\n",
            "float64     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "lgb_cv_scores = []\n",
        "\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\nüî• LightGBM Fold {fold+1}\")\n",
        "\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    model = lgb.train(\n",
        "        lgb_params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[val_data],\n",
        "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    auc = roc_auc_score(y_val, y_pred)\n",
        "    lgb_cv_scores.append(auc)\n",
        "    print(f\" Fold {fold+1} ROC AUC: {auc:.5f}\")\n",
        "\n",
        "print(f\"\\n LightGBM CV: {np.mean(lgb_cv_scores):.5f} ¬± {np.std(lgb_cv_scores):.5f}\")"
      ],
      "metadata": {
        "id": "ZpIoY1P-futo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0efecad-f5cf-4050-83d5-fbd14d59c19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî• LightGBM Fold 1\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[638]\tvalid_0's auc: 0.95562\n",
            " Fold 1 ROC AUC: 0.95562\n",
            "\n",
            "üî• LightGBM Fold 2\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[430]\tvalid_0's auc: 0.95462\n",
            " Fold 2 ROC AUC: 0.95462\n",
            "\n",
            "üî• LightGBM Fold 3\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[404]\tvalid_0's auc: 0.955391\n",
            " Fold 3 ROC AUC: 0.95539\n",
            "\n",
            "üî• LightGBM Fold 4\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[452]\tvalid_0's auc: 0.954927\n",
            " Fold 4 ROC AUC: 0.95493\n",
            "\n",
            "üî• LightGBM Fold 5\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[510]\tvalid_0's auc: 0.955754\n",
            " Fold 5 ROC AUC: 0.95575\n",
            "\n",
            " LightGBM CV: 0.95526 ¬± 0.00043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --q\n",
        "\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Make sure X and y are loaded from previous cells\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 3)\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        **params,\n",
        "        n_estimators=500,                # fixed number (no early stopping needed)\n",
        "        eval_metric='auc',\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    score = np.mean(cross_val_score(model, X, y, cv=3, scoring='roc_auc'))\n",
        "    return score\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)   # 50 trials ~ 30-60 min depending on data size\n",
        "\n",
        "print(\"Best params:\", study.best_params)\n",
        "print(f\"Best CV: {study.best_value:.5f}\")"
      ],
      "metadata": {
        "id": "yomVcbrGfuhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1f4eae-9676-4936-cf16-a8694fe15eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/413.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m409.6/413.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-02-22 02:31:21,494] A new study created in memory with name: no-name-1faaec64-27f4-4722-8640-dd2b867a9a2c\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:31:22] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:31:43] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:32:07] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:32:28,172] Trial 0 finished with value: 0.9540511572155063 and parameters: {'learning_rate': 0.1636817285760767, 'max_depth': 7, 'subsample': 0.963225256094161, 'colsample_bytree': 0.8482593261241237, 'min_child_weight': 8, 'gamma': 0.27987437337614934, 'reg_alpha': 0.29434440958227426, 'reg_lambda': 2.492546435660091}. Best is trial 0 with value: 0.9540511572155063.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:32:29] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:32:48] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:33:05] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:33:24,063] Trial 1 finished with value: 0.955248501670114 and parameters: {'learning_rate': 0.1880061875761464, 'max_depth': 3, 'subsample': 0.5613871626416709, 'colsample_bytree': 0.8527311191545488, 'min_child_weight': 5, 'gamma': 0.24046510671290378, 'reg_alpha': 0.6757475383409495, 'reg_lambda': 2.0044052441590976}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:33:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:34:02] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:34:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:35:17,442] Trial 2 finished with value: 0.9523101176751316 and parameters: {'learning_rate': 0.10737424574617659, 'max_depth': 10, 'subsample': 0.7522862483375303, 'colsample_bytree': 0.8420654797980237, 'min_child_weight': 9, 'gamma': 0.46990303970254993, 'reg_alpha': 0.19542565809208667, 'reg_lambda': 2.2159156205353647}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:35:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:35:47] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:36:16] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:36:45,135] Trial 3 finished with value: 0.9549692163717182 and parameters: {'learning_rate': 0.049459622348893374, 'max_depth': 7, 'subsample': 0.6906107373051917, 'colsample_bytree': 0.997872740476722, 'min_child_weight': 6, 'gamma': 0.44559385807267105, 'reg_alpha': 0.42737837970243797, 'reg_lambda': 1.2033024474061993}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:36:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:37:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:37:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:38:13,935] Trial 4 finished with value: 0.9526524279577416 and parameters: {'learning_rate': 0.17761298916770135, 'max_depth': 7, 'subsample': 0.5837811692227994, 'colsample_bytree': 0.832901752940071, 'min_child_weight': 3, 'gamma': 0.024805654218753415, 'reg_alpha': 0.41020620346420544, 'reg_lambda': 1.1990191864051303}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:38:14] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:38:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:39:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:39:49,617] Trial 5 finished with value: 0.9516783110215039 and parameters: {'learning_rate': 0.18161519510566626, 'max_depth': 8, 'subsample': 0.700323774496842, 'colsample_bytree': 0.6787322209377151, 'min_child_weight': 2, 'gamma': 0.2562945219188607, 'reg_alpha': 0.0941854592769602, 'reg_lambda': 1.2322442430312304}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:39:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:40:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:40:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:40:50,914] Trial 6 finished with value: 0.9551942274862784 and parameters: {'learning_rate': 0.15599990776571443, 'max_depth': 4, 'subsample': 0.6852460096981916, 'colsample_bytree': 0.6682678655861343, 'min_child_weight': 4, 'gamma': 0.4850419034471678, 'reg_alpha': 0.2823392601142485, 'reg_lambda': 2.728325130058842}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:40:51] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:41:29] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:42:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:42:51,202] Trial 7 finished with value: 0.9524965412958686 and parameters: {'learning_rate': 0.09853112027364747, 'max_depth': 11, 'subsample': 0.9174922498889027, 'colsample_bytree': 0.7024857620409907, 'min_child_weight': 9, 'gamma': 0.07236902984827476, 'reg_alpha': 0.0010058057457146274, 'reg_lambda': 2.4004533983035374}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:42:51] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:43:13] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:43:35] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:43:56,052] Trial 8 finished with value: 0.9549237718338223 and parameters: {'learning_rate': 0.13181955863347472, 'max_depth': 5, 'subsample': 0.8181594097280411, 'colsample_bytree': 0.876853943295852, 'min_child_weight': 1, 'gamma': 0.3938982931275828, 'reg_alpha': 0.6126840063947862, 'reg_lambda': 1.3635537105492102}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:43:57] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:44:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:45:25] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:46:09,596] Trial 9 finished with value: 0.9479783831705669 and parameters: {'learning_rate': 0.1727139453514506, 'max_depth': 10, 'subsample': 0.5320034373626232, 'colsample_bytree': 0.7636800327795513, 'min_child_weight': 3, 'gamma': 0.07233121390507175, 'reg_alpha': 0.6981113897423391, 'reg_lambda': 1.3140143377855107}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:46:10] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:46:28] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:46:47] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:47:07,958] Trial 10 finished with value: 0.9548766337705566 and parameters: {'learning_rate': 0.046695490083559416, 'max_depth': 3, 'subsample': 0.5074208778021023, 'colsample_bytree': 0.5491872470237495, 'min_child_weight': 6, 'gamma': 0.1652745826260451, 'reg_alpha': 0.8411203554816944, 'reg_lambda': 1.7632724076963613}. Best is trial 1 with value: 0.955248501670114.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:47:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:47:26] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:47:44] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:48:03,316] Trial 11 finished with value: 0.955335120704178 and parameters: {'learning_rate': 0.19945253409067643, 'max_depth': 3, 'subsample': 0.6082707741469263, 'colsample_bytree': 0.5858710171639269, 'min_child_weight': 4, 'gamma': 0.34766394848828536, 'reg_alpha': 0.9314470085434812, 'reg_lambda': 2.9980605011266217}. Best is trial 11 with value: 0.955335120704178.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:48:03] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:48:21] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:48:39] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:48:58,938] Trial 12 finished with value: 0.9553298247059686 and parameters: {'learning_rate': 0.19656843082977932, 'max_depth': 3, 'subsample': 0.6034875510005975, 'colsample_bytree': 0.5902300467285397, 'min_child_weight': 5, 'gamma': 0.3302763122693221, 'reg_alpha': 0.9434637770552533, 'reg_lambda': 1.7879468304265633}. Best is trial 11 with value: 0.955335120704178.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:48:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:49:21] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:49:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:50:11,410] Trial 13 finished with value: 0.9551257478867329 and parameters: {'learning_rate': 0.13860069314263332, 'max_depth': 5, 'subsample': 0.625923982753981, 'colsample_bytree': 0.5612833380686099, 'min_child_weight': 7, 'gamma': 0.3648355975712181, 'reg_alpha': 0.9911131609540929, 'reg_lambda': 2.877335606966791}. Best is trial 11 with value: 0.955335120704178.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:50:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:50:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:50:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:51:24,130] Trial 14 finished with value: 0.9547999795276033 and parameters: {'learning_rate': 0.1985176169388975, 'max_depth': 5, 'subsample': 0.6104849879749193, 'colsample_bytree': 0.5006904776400782, 'min_child_weight': 4, 'gamma': 0.3178654920702002, 'reg_alpha': 0.9133610286124493, 'reg_lambda': 1.62606989846}. Best is trial 11 with value: 0.955335120704178.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:51:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:51:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:51:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:52:17,427] Trial 15 finished with value: 0.9553738242832807 and parameters: {'learning_rate': 0.09386120969394329, 'max_depth': 3, 'subsample': 0.7972025430485766, 'colsample_bytree': 0.6093894355080446, 'min_child_weight': 5, 'gamma': 0.20504844528432847, 'reg_alpha': 0.8042550809812856, 'reg_lambda': 1.7656565707050176}. Best is trial 15 with value: 0.9553738242832807.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:52:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:52:39] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:53:03] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:53:24,902] Trial 16 finished with value: 0.9539781257201447 and parameters: {'learning_rate': 0.013954641170323717, 'max_depth': 4, 'subsample': 0.8011961892407087, 'colsample_bytree': 0.613085314932816, 'min_child_weight': 7, 'gamma': 0.17102161486250006, 'reg_alpha': 0.8052394405897145, 'reg_lambda': 2.979871860816585}. Best is trial 15 with value: 0.9553738242832807.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:53:25] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:53:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:54:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:54:39,561] Trial 17 finished with value: 0.9551684669710848 and parameters: {'learning_rate': 0.08459945584652398, 'max_depth': 6, 'subsample': 0.8795786953861819, 'colsample_bytree': 0.6318567126129098, 'min_child_weight': 4, 'gamma': 0.19555224184705675, 'reg_alpha': 0.7846482690079308, 'reg_lambda': 2.0886358933140925}. Best is trial 15 with value: 0.9553738242832807.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:54:40] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:54:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:55:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:55:39,790] Trial 18 finished with value: 0.9554213912221217 and parameters: {'learning_rate': 0.07424050330672871, 'max_depth': 4, 'subsample': 0.8376113029962816, 'colsample_bytree': 0.5015486680134138, 'min_child_weight': 1, 'gamma': 0.4060684492552601, 'reg_alpha': 0.513759643712036, 'reg_lambda': 2.583033352725107}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:55:40] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:56:13] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:56:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:57:20,345] Trial 19 finished with value: 0.9547688630374868 and parameters: {'learning_rate': 0.06617406559884094, 'max_depth': 9, 'subsample': 0.8502204926054251, 'colsample_bytree': 0.5009413952853472, 'min_child_weight': 1, 'gamma': 0.42229071696301984, 'reg_alpha': 0.5235625404835875, 'reg_lambda': 2.5909335068667216}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:57:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:57:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:58:04] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:58:23,837] Trial 20 finished with value: 0.954214429565449 and parameters: {'learning_rate': 0.017827209196106863, 'max_depth': 4, 'subsample': 0.9938197971588912, 'colsample_bytree': 0.7501902591201867, 'min_child_weight': 10, 'gamma': 0.11617514710118801, 'reg_alpha': 0.5990055137522292, 'reg_lambda': 2.250225144139919}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:58:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:58:42] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:59:00] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 02:59:16,813] Trial 21 finished with value: 0.9554107829878462 and parameters: {'learning_rate': 0.12025325007121909, 'max_depth': 3, 'subsample': 0.7649475440306669, 'colsample_bytree': 0.5398709581925858, 'min_child_weight': 2, 'gamma': 0.355850971921867, 'reg_alpha': 0.8506675681960967, 'reg_lambda': 2.736820488993679}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:59:17] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:59:38] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [02:59:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:00:17,886] Trial 22 finished with value: 0.9553947333290548 and parameters: {'learning_rate': 0.11946272529232166, 'max_depth': 4, 'subsample': 0.7695288820638432, 'colsample_bytree': 0.5425248111770504, 'min_child_weight': 2, 'gamma': 0.29541020337559487, 'reg_alpha': 0.7237678911487206, 'reg_lambda': 2.6583705915543088}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:00:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:00:44] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:01:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:01:37,375] Trial 23 finished with value: 0.9550326926711898 and parameters: {'learning_rate': 0.1163279094832196, 'max_depth': 6, 'subsample': 0.7472227197821215, 'colsample_bytree': 0.5323549157045612, 'min_child_weight': 2, 'gamma': 0.38796285054993007, 'reg_alpha': 0.7169988557132352, 'reg_lambda': 2.696093991686667}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:01:37] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:01:57] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:02:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:02:37,591] Trial 24 finished with value: 0.9553877241073727 and parameters: {'learning_rate': 0.07588465183070026, 'max_depth': 4, 'subsample': 0.7577557738389895, 'colsample_bytree': 0.5407559100374113, 'min_child_weight': 2, 'gamma': 0.2911905400608943, 'reg_alpha': 0.5235248692179848, 'reg_lambda': 2.7627035056863343}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:02:38] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:03:02] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:03:23] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:03:48,512] Trial 25 finished with value: 0.9548818614352993 and parameters: {'learning_rate': 0.12445175828477584, 'max_depth': 6, 'subsample': 0.9073054340078578, 'colsample_bytree': 0.6382131705057157, 'min_child_weight': 1, 'gamma': 0.39985349630426564, 'reg_alpha': 0.6130601858541138, 'reg_lambda': 2.412525582763139}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:03:49] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:04:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:05:15] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:05:58,478] Trial 26 finished with value: 0.9505657295578264 and parameters: {'learning_rate': 0.14882341501654944, 'max_depth': 12, 'subsample': 0.8329111820583857, 'colsample_bytree': 0.5165884077295547, 'min_child_weight': 3, 'gamma': 0.30722551487349914, 'reg_alpha': 0.4785855440625892, 'reg_lambda': 2.5659302212196256}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:05:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:06:22] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:06:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:07:10,051] Trial 27 finished with value: 0.9552378722525745 and parameters: {'learning_rate': 0.11197826293265215, 'max_depth': 5, 'subsample': 0.6562846742992813, 'colsample_bytree': 0.5704983858573686, 'min_child_weight': 2, 'gamma': 0.4367903468368982, 'reg_alpha': 0.858641959647177, 'reg_lambda': 2.256426015163655}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:07:10] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:07:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:07:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:08:11,667] Trial 28 finished with value: 0.9553184751518836 and parameters: {'learning_rate': 0.05983671018789252, 'max_depth': 4, 'subsample': 0.7684345612725354, 'colsample_bytree': 0.7115829036002216, 'min_child_weight': 1, 'gamma': 0.35636019031481797, 'reg_alpha': 0.7453264868635086, 'reg_lambda': 2.8363758874635825}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:08:12] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:08:48] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:09:21] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:09:56,712] Trial 29 finished with value: 0.9549006642920198 and parameters: {'learning_rate': 0.0319442019257415, 'max_depth': 8, 'subsample': 0.7252200265002624, 'colsample_bytree': 0.9708256561386572, 'min_child_weight': 3, 'gamma': 0.2734906311415442, 'reg_alpha': 0.36735443629218933, 'reg_lambda': 2.472165162235658}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:09:57] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:10:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:10:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:11:11,040] Trial 30 finished with value: 0.9549994882078323 and parameters: {'learning_rate': 0.08920979210655333, 'max_depth': 6, 'subsample': 0.8728514939167986, 'colsample_bytree': 0.7971473157980208, 'min_child_weight': 2, 'gamma': 0.24071716766152473, 'reg_alpha': 0.572968951868796, 'reg_lambda': 2.625592456883896}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:11:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:11:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:11:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:12:11,058] Trial 31 finished with value: 0.9554131886971126 and parameters: {'learning_rate': 0.07491781362563849, 'max_depth': 4, 'subsample': 0.7882374784442511, 'colsample_bytree': 0.5373744673443507, 'min_child_weight': 2, 'gamma': 0.30103493450672536, 'reg_alpha': 0.31228901036283113, 'reg_lambda': 2.802189597606228}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:12:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:12:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:12:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:13:11,680] Trial 32 finished with value: 0.9554202374429438 and parameters: {'learning_rate': 0.07428997315266296, 'max_depth': 4, 'subsample': 0.7827795796540397, 'colsample_bytree': 0.5350067434397225, 'min_child_weight': 1, 'gamma': 0.2938591283948444, 'reg_alpha': 0.2953130278074237, 'reg_lambda': 2.8550009136361103}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:13:12] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:13:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:13:49] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:14:05,668] Trial 33 finished with value: 0.9552100439978487 and parameters: {'learning_rate': 0.07027272931212078, 'max_depth': 3, 'subsample': 0.7998326987368226, 'colsample_bytree': 0.5762955455923295, 'min_child_weight': 1, 'gamma': 0.3753430971863857, 'reg_alpha': 0.27477704707857725, 'reg_lambda': 2.8462191323045434}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:14:06] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:14:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:14:55] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:15:17,747] Trial 34 finished with value: 0.9554011624204438 and parameters: {'learning_rate': 0.05503476673377854, 'max_depth': 5, 'subsample': 0.7231149727640275, 'colsample_bytree': 0.531242653313519, 'min_child_weight': 1, 'gamma': 0.3367001682821772, 'reg_alpha': 0.18850445399905189, 'reg_lambda': 2.886936273947851}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:15:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:15:34] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:15:52] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:16:08,798] Trial 35 finished with value: 0.9552959229712431 and parameters: {'learning_rate': 0.07996961860134146, 'max_depth': 3, 'subsample': 0.9370340418265116, 'colsample_bytree': 0.649160314180957, 'min_child_weight': 3, 'gamma': 0.448211125376488, 'reg_alpha': 0.18800206341444925, 'reg_lambda': 2.5208441486457356}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:16:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:16:28] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:16:51] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:17:10,675] Trial 36 finished with value: 0.9551312762001466 and parameters: {'learning_rate': 0.04185244512231874, 'max_depth': 4, 'subsample': 0.8569296214012222, 'colsample_bytree': 0.6041797358579255, 'min_child_weight': 2, 'gamma': 0.2625500902876208, 'reg_alpha': 0.34069178018479185, 'reg_lambda': 2.7791637907370945}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:17:11] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:17:39] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:18:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:18:35,914] Trial 37 finished with value: 0.9548015136434677 and parameters: {'learning_rate': 0.10135908255586301, 'max_depth': 7, 'subsample': 0.7818106216941464, 'colsample_bytree': 0.517174152806045, 'min_child_weight': 1, 'gamma': 0.49939342196121284, 'reg_alpha': 0.25048565571576337, 'reg_lambda': 2.4149951752940386}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:18:36] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:18:58] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:19:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:19:43,115] Trial 38 finished with value: 0.955317869668491 and parameters: {'learning_rate': 0.10389778186739548, 'max_depth': 5, 'subsample': 0.8268646583535879, 'colsample_bytree': 0.5616516460870015, 'min_child_weight': 3, 'gamma': 0.22326725966828842, 'reg_alpha': 0.4377514642053885, 'reg_lambda': 2.153456582308073}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:19:43] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:20:01] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:20:18] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:20:35,935] Trial 39 finished with value: 0.9551775507509248 and parameters: {'learning_rate': 0.06645265299630289, 'max_depth': 3, 'subsample': 0.7273866602237496, 'colsample_bytree': 0.502813859865591, 'min_child_weight': 2, 'gamma': 0.40935076682732985, 'reg_alpha': 0.10728803094167255, 'reg_lambda': 2.353834313700951}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:20:36] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:20:59] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:21:23] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:21:44,018] Trial 40 finished with value: 0.9546283180042225 and parameters: {'learning_rate': 0.027117431542842033, 'max_depth': 4, 'subsample': 0.6747883140300569, 'colsample_bytree': 0.8985091021600898, 'min_child_weight': 1, 'gamma': 0.45355001526616445, 'reg_alpha': 0.35499978728963655, 'reg_lambda': 2.930863538028923}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:21:44] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:22:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:22:33] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:22:55,887] Trial 41 finished with value: 0.9553975665141209 and parameters: {'learning_rate': 0.052550305005992684, 'max_depth': 5, 'subsample': 0.70828778433233, 'colsample_bytree': 0.5347409511882362, 'min_child_weight': 1, 'gamma': 0.34704999174382534, 'reg_alpha': 0.099948057012675, 'reg_lambda': 2.7678304378515803}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:22:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:23:21] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:23:44] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:24:07,177] Trial 42 finished with value: 0.9554005878476568 and parameters: {'learning_rate': 0.05694408390307516, 'max_depth': 5, 'subsample': 0.7222044883753178, 'colsample_bytree': 0.5277773545083687, 'min_child_weight': 1, 'gamma': 0.3179858824530544, 'reg_alpha': 0.2073292007038939, 'reg_lambda': 2.901018542413294}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:24:07] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:24:30] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:24:50] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:25:12,705] Trial 43 finished with value: 0.9551033699128002 and parameters: {'learning_rate': 0.04059890895149426, 'max_depth': 4, 'subsample': 0.7479193055272534, 'colsample_bytree': 0.5590596304890332, 'min_child_weight': 2, 'gamma': 0.2872164708171194, 'reg_alpha': 0.15990683814256657, 'reg_lambda': 2.6742891601864414}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:25:13] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:25:31] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:25:48] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:26:07,276] Trial 44 finished with value: 0.9552393084287614 and parameters: {'learning_rate': 0.07547071806875974, 'max_depth': 3, 'subsample': 0.6604232531917091, 'colsample_bytree': 0.6728794469237831, 'min_child_weight': 1, 'gamma': 0.3411966708725004, 'reg_alpha': 0.04893928930342989, 'reg_lambda': 2.838585595307996}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:26:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:26:27] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:26:46] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:27:07,646] Trial 45 finished with value: 0.9554168961676591 and parameters: {'learning_rate': 0.09176587315055211, 'max_depth': 4, 'subsample': 0.8137096489603276, 'colsample_bytree': 0.5770015945305187, 'min_child_weight': 3, 'gamma': 0.3800005745905491, 'reg_alpha': 0.306028636861179, 'reg_lambda': 2.9759550544534004}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:27:08] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:27:24] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:27:41] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:27:57,904] Trial 46 finished with value: 0.9553585809758035 and parameters: {'learning_rate': 0.09699551606120196, 'max_depth': 3, 'subsample': 0.8204302210574802, 'colsample_bytree': 0.5949628170701949, 'min_child_weight': 3, 'gamma': 0.372924166349882, 'reg_alpha': 0.3266894779205532, 'reg_lambda': 2.9873515328510205}. Best is trial 18 with value: 0.9554213912221217.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:27:58] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:28:19] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:28:37] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:28:55,583] Trial 47 finished with value: 0.9554353018184232 and parameters: {'learning_rate': 0.08667979144229895, 'max_depth': 4, 'subsample': 0.8896902281272561, 'colsample_bytree': 0.5714628753919909, 'min_child_weight': 4, 'gamma': 0.4195090366911791, 'reg_alpha': 0.40330427127836105, 'reg_lambda': 2.7337414230706445}. Best is trial 47 with value: 0.9554353018184232.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:28:56] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:29:20] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:29:45] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:30:08,446] Trial 48 finished with value: 0.9551348192982386 and parameters: {'learning_rate': 0.08892037585898659, 'max_depth': 6, 'subsample': 0.8953218111954117, 'colsample_bytree': 0.6183022894881035, 'min_child_weight': 4, 'gamma': 0.4721281028432263, 'reg_alpha': 0.4056816970453912, 'reg_lambda': 1.0798309534099189}. Best is trial 47 with value: 0.9554353018184232.\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:30:09] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:30:28] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:200: UserWarning: [03:30:47] WARNING: /__w/xgboost/xgboost/src/learner.cc:782: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2026-02-22 03:31:07,336] Trial 49 finished with value: 0.9553607531304836 and parameters: {'learning_rate': 0.08449834144736795, 'max_depth': 4, 'subsample': 0.8467660961375398, 'colsample_bytree': 0.6996645595422627, 'min_child_weight': 5, 'gamma': 0.43358989797898434, 'reg_alpha': 0.4460917505645804, 'reg_lambda': 2.579799873627859}. Best is trial 47 with value: 0.9554353018184232.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'learning_rate': 0.08667979144229895, 'max_depth': 4, 'subsample': 0.8896902281272561, 'colsample_bytree': 0.5714628753919909, 'min_child_weight': 4, 'gamma': 0.4195090366911791, 'reg_alpha': 0.40330427127836105, 'reg_lambda': 2.7337414230706445}\n",
            "Best CV: 0.95544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study.best_params.copy()\n",
        "best_params.pop('n_estimators', None)\n",
        "best_params['n_estimators'] = 1000  # we'll use early stopping to find optimal number\n",
        "\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    early_stopping_rounds=50,\n",
        "    eval_metric='auc',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# We need an eval_set for early stopping ‚Äî split a small validation set from training\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Clean column names\n",
        "X.columns = X.columns.str.replace(' ', '_').str.replace('^', '_').str.replace('-', '_')\n",
        "X_test.columns = X_test.columns.str.replace(' ', '_').str.replace('^', '_').str.replace('-', '_')\n",
        "# Splitting the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "final_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"Best iteration: {final_model.best_iteration}\")"
      ],
      "metadata": {
        "id": "0G44Qrcxftaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca73fc6-445e-408f-ff3a-23cd4951f5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best iteration: 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From your correlation list:\n",
        "top_features = ['Thallium', 'Chest_pain_type', 'Exercise_angina',\n",
        "                'Number of vessels fluro', 'ST depression', 'Slope of ST', 'Max HR']\n",
        "\n",
        "# Interactions (multiplicative)\n",
        "X['thal_x_chest'] = X['Thallium'] * X['Chest_pain_type']\n",
        "X['exang_x_vessels'] = X['Exercise_angina'] * X['Number_of_vessels_fluro']\n",
        "X['stdep_x_slope'] = X['ST_depression'] * X['Slope_of_ST']\n",
        "X['age_x_maxhr'] = X['Age'] * X['Max_HR']\n",
        "\n",
        "# Ratios (if denominator > 0)\n",
        "X['bp_per_age'] = X['BP'] / (X['Age'] + 1)\n",
        "X['chol_per_age'] = X['Cholesterol'] / (X['Age'] + 1)\n",
        "\n",
        "# Polynomials (degree=2) for top 3\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
        "top3 = X[['Thallium', 'Chest_pain_type', 'Exercise_angina']]\n",
        "poly_features = poly.fit_transform(top3)\n",
        "poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(top3.columns))\n",
        "X = pd.concat([X, poly_df], axis=1)"
      ],
      "metadata": {
        "id": "qxbvTzpKfhAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"type(y): {type(y)}\")\n",
        "print(f\"type(y_train): {type(y_train)}\")\n",
        "print(f\"type(y_val): {type(y_val)}\")"
      ],
      "metadata": {
        "id": "QoJN-BDdr83v",
        "outputId": "b296371a-f9da-4674-b57f-5906bc8e1381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(y): <class 'pandas.core.series.Series'>\n",
            "type(y_train): <class 'pandas.core.series.Series'>\n",
            "type(y_val): <class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if y is a DataFrame\n",
        "if isinstance(y, pd.DataFrame):\n",
        "    print(\"y is a DataFrame! Converting to Series...\")\n",
        "    y = y.iloc[:, 0]   # take the first (and only) column as a Series\n",
        "\n",
        "# Also check y_train and y_val after split\n",
        "if isinstance(y_train, pd.DataFrame):\n",
        "    y_train = y_train.iloc[:, 0]\n",
        "if isinstance(y_val, pd.DataFrame):\n",
        "    y_val = y_val.iloc[:, 0]"
      ],
      "metadata": {
        "id": "YkCJDp1_rfwy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train dtypes:\")\n",
        "print(X_train.dtypes.value_counts())\n",
        "print(\"\\nColumns with object dtype:\")\n",
        "print(X_train.select_dtypes(include=['object']).columns.tolist())\n",
        "print(\"\\nAny infinite values?\")\n",
        "print(np.isinf(X_train).any().any())\n",
        "print(\"\\nAny NaN values?\")\n",
        "print(X_train.isna().any().any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7EpvpGzmtvq",
        "outputId": "e966d33f-96ac-467a-e352-639239afbb17"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train dtypes:\n",
            "int64      15\n",
            "float64    13\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Columns with object dtype:\n",
            "[]\n",
            "\n",
            "Any infinite values?\n",
            "False\n",
            "\n",
            "Any NaN values?\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# import lightgbm as lgb\n",
        "# import numpy as np\n",
        "\n",
        "# # --- Ensure y is a Series ---\n",
        "# if isinstance(y, pd.DataFrame):\n",
        "#     y = y.iloc[:, 0]\n",
        "\n",
        "# # --- XGBoost best params ---\n",
        "# best_xgb_params = study.best_params.copy()\n",
        "# best_xgb_params.pop('n_estimators', None)\n",
        "\n",
        "# # --- LightGBM params (defaults) ---\n",
        "# best_lgb_params = {\n",
        "#     'learning_rate': 0.05,\n",
        "#     'num_leaves': 31,\n",
        "#     'max_depth': -1,\n",
        "#     'feature_fraction': 0.8,\n",
        "#     'bagging_fraction': 0.8,\n",
        "#     'bagging_freq': 5,\n",
        "#     'min_child_samples': 20,\n",
        "#     'reg_alpha': 0.0,\n",
        "#     'reg_lambda': 0.0,\n",
        "#     'random_state': 42,\n",
        "#     'n_jobs': -1,\n",
        "#     'verbose': -1\n",
        "# }\n",
        "\n",
        "# # Split data for early stopping\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "# # --- CONVERT TO NUMPY ARRAYS (this is the key fix) ---\n",
        "# X_train_np = X_train.values.astype('float32')\n",
        "# X_val_np = X_val.values.astype('float32')\n",
        "# X_full_np = X.values.astype('float32')\n",
        "# X_test_np = X_test.values.astype('float32')\n",
        "\n",
        "# y_train_np = y_train.values.ravel()\n",
        "# y_val_np = y_val.values.ravel()\n",
        "# y_full_np = y.values.ravel()\n",
        "\n",
        "# # Train XGBoost with early stopping (using numpy arrays)\n",
        "# xgb_final = xgb.XGBClassifier(**best_xgb_params, n_estimators=1000,\n",
        "#                               early_stopping_rounds=50, random_state=42, n_jobs=-1)\n",
        "# xgb_final.fit(X_train_np, y_train_np, eval_set=[(X_val_np, y_val_np)], verbose=False)\n",
        "\n",
        "# # Train LightGBM with early stopping (using numpy arrays)\n",
        "# lgb_final = lgb.LGBMClassifier(**best_lgb_params, n_estimators=1000,\n",
        "#                                early_stopping_rounds=50, random_state=42, n_jobs=-1)\n",
        "# lgb_final.fit(X_train_np, y_train_np, eval_set=[(X_val_np, y_val_np)], verbose=False)\n",
        "\n",
        "# # Get optimal number of trees\n",
        "# xgb_best_iter = xgb_final.best_iteration\n",
        "# lgb_best_iter = lgb_final.best_iteration_\n",
        "\n",
        "# print(f\"XGBoost optimal trees: {xgb_best_iter}\")\n",
        "# print(f\"LightGBM optimal trees: {lgb_best_iter}\")\n",
        "\n",
        "# # Retrain on full data with optimal trees (using numpy arrays)\n",
        "# xgb_full = xgb.XGBClassifier(**best_xgb_params, n_estimators=xgb_best_iter, random_state=42, n_jobs=-1)\n",
        "# xgb_full.fit(X_full_np, y_full_np)\n",
        "\n",
        "# lgb_full = lgb.LGBMClassifier(**best_lgb_params, n_estimators=lgb_best_iter, random_state=42, n_jobs=-1)\n",
        "# lgb_full.fit(X_full_np, y_full_np)\n",
        "\n",
        "# # Predict\n",
        "# xgb_preds = xgb_full.predict_proba(X_test_np)[:, 1]\n",
        "# lgb_preds = lgb_full.predict_proba(X_test_np)[:, 1]\n",
        "\n",
        "# # Ensemble\n",
        "# ensemble_preds = (xgb_preds + lgb_preds) / 2\n",
        "\n",
        "# # Submission\n",
        "# submission = pd.DataFrame({'id': test_ids, 'Heart Disease': ensemble_preds})\n",
        "# submission.to_csv('ensemble_xgb_lgb.csv', index=False)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('ensemble_xgb_lgb.csv')"
      ],
      "metadata": {
        "id": "OvCWregbqYff"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# --- Ensure y is a Series ---\n",
        "if isinstance(y, pd.DataFrame):\n",
        "    y = y.iloc[:, 0]\n",
        "\n",
        "# --- XGBoost best params ---\n",
        "best_xgb_params = study.best_params.copy()\n",
        "best_xgb_params.pop('n_estimators', None)\n",
        "\n",
        "# --- LightGBM params (defaults) ---\n",
        "best_lgb_params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_child_samples': 20,\n",
        "    'reg_alpha': 0.0,\n",
        "    'reg_lambda': 0.0,\n",
        "    'random_state': 42,          # included\n",
        "    'n_jobs': -1,                 # included\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# Split data for early stopping\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "# --- CONVERT TO NUMPY ARRAYS (the key fix) ---\n",
        "X_train_np = X_train.values.astype('float32')\n",
        "X_val_np = X_val.values.astype('float32')\n",
        "X_full_np = X.values.astype('float32')\n",
        "X_test_np = X_test.values.astype('float32')\n",
        "\n",
        "y_train_np = y_train.values.ravel()\n",
        "y_val_np = y_val.values.ravel()\n",
        "y_full_np = y.values.ravel()\n",
        "\n",
        "# Train XGBoost with early stopping\n",
        "xgb_final = xgb.XGBClassifier(**best_xgb_params, n_estimators=1000,\n",
        "                              early_stopping_rounds=50, random_state=42, n_jobs=-1)\n",
        "xgb_final.fit(X_train_np, y_train_np, eval_set=[(X_val_np, y_val_np)], verbose=False)\n",
        "\n",
        "# Train LightGBM with early stopping ‚Äî NO DUPLICATE ARGS\n",
        "lgb_final = lgb.LGBMClassifier(**best_lgb_params, n_estimators=1000, early_stopping_rounds=50)\n",
        "lgb_final.fit(X_train_np, y_train_np, eval_set=[(X_val_np, y_val_np)])\n",
        "\n",
        "# Get optimal number of trees\n",
        "xgb_best_iter = xgb_final.best_iteration\n",
        "lgb_best_iter = lgb_final.best_iteration_\n",
        "\n",
        "print(f\"XGBoost optimal trees: {xgb_best_iter}\")\n",
        "print(f\"LightGBM optimal trees: {lgb_best_iter}\")\n",
        "\n",
        "# Retrain on full data with optimal trees\n",
        "xgb_full = xgb.XGBClassifier(**best_xgb_params, n_estimators=xgb_best_iter, random_state=42, n_jobs=-1)\n",
        "xgb_full.fit(X_full_np, y_full_np)\n",
        "\n",
        "# LightGBM retrain ‚Äî NO DUPLICATE ARGS\n",
        "lgb_full = lgb.LGBMClassifier(**best_lgb_params, n_estimators=lgb_best_iter)\n",
        "lgb_full.fit(X_full_np, y_full_np)\n",
        "\n",
        "# Predict\n",
        "xgb_preds = xgb_full.predict_proba(X_test_np)[:, 1]\n",
        "lgb_preds = lgb_full.predict_proba(X_test_np)[:, 1]\n",
        "\n",
        "# Ensemble\n",
        "ensemble_preds = (xgb_preds + lgb_preds) / 2\n",
        "\n",
        "# Submission\n",
        "submission = pd.DataFrame({'id': test_ids, 'Heart Disease': ensemble_preds})\n",
        "submission.to_csv('ensemble_xgb_lgb.csv', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('ensemble_xgb_lgb.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "h8IkQHZzsMKV",
        "outputId": "8c36649f-64ca-4f50-ecb4-d2ec70a7d5e6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost optimal trees: 601\n",
            "LightGBM optimal trees: 441\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Feature shape mismatch, expected: 28, got 13",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-564710972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mxgb_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mlgb_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1919\u001b[0m             \u001b[0mclass_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_predt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m         class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1922\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m             \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1447\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2863\u001b[0m                 )\n\u001b[1;32m   2864\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2865\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2866\u001b[0m                     \u001b[0;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                     \u001b[0;34mf\"got {data.shape[1]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 28, got 13"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvpavOJXfJA4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import catboost as cb\n",
        "\n",
        "base_models = [\n",
        "    ('xgb', xgb.XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)),\n",
        "    ('lgb', lgb.LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=31, random_state=42)),\n",
        "    ('cb', cb.CatBoostClassifier(iterations=500, learning_rate=0.05, verbose=0, random_state=42))\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=5)\n",
        "stack.fit(X, y)\n",
        "stack_preds = stack.predict_proba(X_test)[:, 1]\n",
        "\n",
        "submission['Heart Disease'] = stack_preds\n",
        "submission.to_csv('stacking_submission.csv', index=False)"
      ]
    }
  ]
}